@misc{machlab_llm_2024,
	title = {{LLM} {In}-{Context} {Recall} is {Prompt} {Dependent}},
	url = {http://arxiv.org/abs/2404.08865},
	abstract = {The proliferation of Large Language Models (LLMs) highlights the critical importance of conducting thorough evaluations to discern their comparative advantages, limitations, and optimal use cases. Particularly important is assessing their capacity to accurately retrieve information included in a given prompt. A model’s ability to do this significantly influences how effectively it can utilize contextual details, thus impacting its practical efficacy and dependability in real-world applications. Our research analyzes the in-context recall performance of various LLMs using the needle-in-a-haystack method. In this approach, a factoid (the “needle”) is embedded within a block of filler text (the “haystack”), which the model is asked to retrieve. We assess the recall performance of each model across various haystack lengths and with varying needle placements to identify performance patterns. This study demonstrates that an LLM’s recall capability is not only contingent upon the prompt’s content but also may be compromised by biases in its training data. Conversely, adjustments to model architecture, training strategy, or fine-tuning can improve performance. Our analysis provides insight into LLM behavior, offering direction for the development of more effective applications of LLMs.},
	language = {en},
	urldate = {2024-04-26},
	publisher = {arXiv},
	author = {Machlab, Daniel and Battle, Rick},
	month = apr,
	year = {2024},
	note = {arXiv:2404.08865 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{page_prisma_2021,
	title = {The {PRISMA} 2020 statement: an updated guideline for reporting systematic reviews},
	volume = {10},
	issn = {2046-4053},
	shorttitle = {The {PRISMA} 2020 statement},
	url = {https://doi.org/10.1186/s13643-021-01626-4},
	doi = {10.1186/s13643-021-01626-4},
	number = {1},
	urldate = {2022-11-03},
	journal = {Systematic Reviews},
	author = {Page, Matthew J. and McKenzie, Joanne E. and Bossuyt, Patrick M. and Boutron, Isabelle and Hoffmann, Tammy C. and Mulrow, Cynthia D. and Shamseer, Larissa and Tetzlaff, Jennifer M. and Akl, Elie A. and Brennan, Sue E. and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M. and Hróbjartsson, Asbjørn and Lalu, Manoj M. and Li, Tianjing and Loder, Elizabeth W. and Mayo-Wilson, Evan and McDonald, Steve and McGuinness, Luke A. and Stewart, Lesley A. and Thomas, James and Tricco, Andrea C. and Welch, Vivian A. and Whiting, Penny and Moher, David},
	month = mar,
	year = {2021},
	pages = {89},
}

@article{blaizot_using_2022,
	title = {Using artificial intelligence methods for systematic review in health sciences: {A} systematic review},
	volume = {13},
	issn = {1759-2887},
	shorttitle = {Using artificial intelligence methods for systematic review in health sciences},
	doi = {10.1002/jrsm.1553},
	abstract = {The exponential increase in published articles makes a thorough and expedient review of literature increasingly challenging. This review delineated automated tools and platforms that employ artificial intelligence (AI) approaches and evaluated the reported benefits and challenges in using such methods. A search was conducted in 4 databases (Medline, Embase, CDSR, and Epistemonikos) up to April 2021 for systematic reviews and other related reviews implementing AI methods. To be included, the review must use any form of AI method, including machine learning, deep learning, neural network, or any other applications used to enable the full or semi-autonomous performance of one or more stages in the development of evidence synthesis. Twelve reviews were included, using nine different tools to implement 15 different AI methods. Eleven methods were used in the screening stages of the review (73\%). The rest were divided: two in data extraction (13\%) and two in risk of bias assessment (13\%). The ambiguous benefits of the data extractions, combined with the reported advantages from 10 reviews, indicating that AI platforms have taken hold with varying success in evidence synthesis. However, the results are qualified by the reliance on the self-reporting of the review authors. Extensive human validation still appears required at this stage in implementing AI methods, though further evaluation is required to define the overall contribution of such platforms in enhancing efficiency and quality in evidence synthesis.},
	language = {eng},
	number = {3},
	journal = {Research Synthesis Methods},
	author = {Blaizot, Aymeric and Veettil, Sajesh K. and Saidoung, Pantakarn and Moreno-Garcia, Carlos Francisco and Wiratunga, Nirmalie and Aceves-Martins, Magaly and Lai, Nai Ming and Chaiyakunapruk, Nathorn},
	month = may,
	year = {2022},
	pmid = {35174972},
	keywords = {artificial intelligence, Artificial Intelligence, evidence synthesis, Humans, machine learning, Machine Learning, Medicine, systematic reviews, Systematic Reviews as Topic},
	pages = {353--362},
}

@article{fabiano_how_nodate,
	title = {How to optimize the systematic review process using {AI} tools},
	volume = {n/a},
	copyright = {© 2024 The Authors. JCPP Advances published by John Wiley \& Sons Ltd on behalf of Association for Child and Adolescent Mental Health.},
	issn = {2692-9384},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jcv2.12234},
	doi = {10.1002/jcv2.12234},
	abstract = {Systematic reviews are a cornerstone for synthesizing the available evidence on a given topic. They simultaneously allow for gaps in the literature to be identified and provide direction for future research. However, due to the ever-increasing volume and complexity of the available literature, traditional methods for conducting systematic reviews are less efficient and more time-consuming. Numerous artificial intelligence (AI) tools are being released with the potential to optimize efficiency in academic writing and assist with various stages of the systematic review process including developing and refining search strategies, screening titles and abstracts for inclusion or exclusion criteria, extracting essential data from studies and summarizing findings. Therefore, in this article we provide an overview of the currently available tools and how they can be incorporated into the systematic review process to improve efficiency and quality of research synthesis. We emphasize that authors must report all AI tools that have been used at each stage to ensure replicability as part of reporting in methods.},
	language = {en},
	number = {n/a},
	urldate = {2024-05-30},
	journal = {JCPP Advances},
	author = {Fabiano, Nicholas and Gupta, Arnav and Bhambra, Nishaant and Luu, Brandon and Wong, Stanley and Maaz, Muhammad and Fiedorowicz, Jess G. and Smith, Andrew L. and Solmi, Marco},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jcv2.12234},
	keywords = {artificial intelligence, ChatGPT, evidence synthesis, large-language models, systematic review},
	pages = {e12234},
	year = 2024
}

@article{van_de_schoot_open_2021,
	title = {An open source machine learning framework for efficient and transparent systematic reviews},
	volume = {3},
	copyright = {2021 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00287-7},
	doi = {10.1038/s42256-020-00287-7},
	abstract = {To help researchers conduct a systematic review or meta-analysis as efficiently and transparently as possible, we designed a tool to accelerate the step of screening titles and abstracts. For many tasks—including but not limited to systematic reviews and meta-analyses—the scientific literature needs to be checked systematically. Scholars and practitioners currently screen thousands of studies by hand to determine which studies to include in their review or meta-analysis. This is error prone and inefficient because of extremely imbalanced data: only a fraction of the screened studies is relevant. The future of systematic reviewing will be an interaction with machine learning algorithms to deal with the enormous increase of available text. We therefore developed an open source machine learning-aided pipeline applying active learning: ASReview. We demonstrate by means of simulation studies that active learning can yield far more efficient reviewing than manual reviewing while providing high quality. Furthermore, we describe the options of the free and open source research software and present the results from user experience tests. We invite the community to contribute to open source projects such as our own that provide measurable and reproducible improvements over current practice.},
	language = {en},
	number = {2},
	urldate = {2024-05-30},
	journal = {Nature Machine Intelligence},
	author = {van de Schoot, Rens and de Bruin, Jonathan and Schram, Raoul and Zahedi, Parisa and de Boer, Jan and Weijdema, Felix and Kramer, Bianca and Huijts, Martijn and Hoogerwerf, Maarten and Ferdinands, Gerbrich and Harkema, Albert and Willemsen, Joukje and Ma, Yongchao and Fang, Qixiang and Hindriks, Sybren and Tummers, Lars and Oberski, Daniel L.},
	month = feb,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Computer science, Medical research, SARS-CoV-2},
	pages = {125--133},
}

@article{de_la_torre-lopez_artificial_2023,
	title = {Artificial intelligence to automate the systematic review of scientific literature},
	volume = {105},
	issn = {1436-5057},
	url = {https://doi.org/10.1007/s00607-023-01181-x},
	doi = {10.1007/s00607-023-01181-x},
	abstract = {Artificial intelligence (AI) has acquired notorious relevance in modern computing as it effectively solves complex tasks traditionally done by humans. AI provides methods to represent and infer knowledge, efficiently manipulate texts and learn from vast amount of data. These characteristics are applicable in many activities that human find laborious or repetitive, as is the case of the analysis of scientific literature. Manually preparing and writing a systematic literature review (SLR) takes considerable time and effort, since it requires planning a strategy, conducting the literature search and analysis, and reporting the findings. Depending on the area under study, the number of papers retrieved can be of hundreds or thousands, meaning that filtering those relevant ones and extracting the key information becomes a costly and error-prone process. However, some of the involved tasks are repetitive and, therefore, subject to automation by means of AI. In this paper, we present a survey of AI techniques proposed in the last 15 years to help researchers conduct systematic analyses of scientific literature. We describe the tasks currently supported, the types of algorithms applied, and available tools proposed in 34 primary studies. This survey also provides a historical perspective of the evolution of the field and the role that humans can play in an increasingly automated SLR process.},
	language = {en},
	number = {10},
	urldate = {2024-05-30},
	journal = {Computing},
	author = {de la Torre-López, José and Ramírez, Aurora and Romero, José Raúl},
	month = oct,
	year = {2023},
	keywords = {68T01 General topics in artificial intelligence, Artificial intelligence, Machine learning, Survey, Systematic literature review},
	pages = {2171--2194},
}

@article{dijk_artificial_2023,
	title = {Artificial intelligence in systematic reviews: promising when appropriately used},
	volume = {13},
	copyright = {© Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY. Published by BMJ.. https://creativecommons.org/licenses/by/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution 4.0 Unported (CC BY 4.0) license, which permits others to copy, redistribute, remix, transform and build upon this work for any purpose, provided the original work is properly cited, a link to the licence is given, and indication of whether changes were made. See: https://creativecommons.org/licenses/by/4.0/.},
	issn = {2044-6055, 2044-6055},
	shorttitle = {Artificial intelligence in systematic reviews},
	url = {https://bmjopen.bmj.com/content/13/7/e072254},
	doi = {10.1136/bmjopen-2023-072254},
	abstract = {Background Systematic reviews provide a structured overview of the available evidence in medical-scientific research. However, due to the increasing medical-scientific research output, it is a time-consuming task to conduct systematic reviews. To accelerate this process, artificial intelligence (AI) can be used in the review process. In this communication paper, we suggest how to conduct a transparent and reliable systematic review using the AI tool ‘ASReview’ in the title and abstract screening.
Methods Use of the AI tool consisted of several steps. First, the tool required training of its algorithm with several prelabelled articles prior to screening. Next, using a researcher-in-the-loop algorithm, the AI tool proposed the article with the highest probability of being relevant. The reviewer then decided on relevancy of each article proposed. This process was continued until the stopping criterion was reached. All articles labelled relevant by the reviewer were screened on full text.
Results Considerations to ensure methodological quality when using AI in systematic reviews included: the choice of whether to use AI, the need of both deduplication and checking for inter-reviewer agreement, how to choose a stopping criterion and the quality of reporting. Using the tool in our review resulted in much time saved: only 23\% of the articles were assessed by the reviewer.
Conclusion The AI tool is a promising innovation for the current systematic reviewing practice, as long as it is appropriately used and methodological quality can be assured.
PROSPERO registration number CRD42022283952.},
	language = {en},
	number = {7},
	urldate = {2024-05-30},
	journal = {BMJ Open},
	author = {Dijk, Sanne H. B. van and Brusse-Keizer, Marjolein G. J. and Bucsán, Charlotte C. and Palen, Job van der and Doggen, Carine J. M. and Lenferink, Anke},
	month = jul,
	year = {2023},
	pmid = {37419641},
	note = {Publisher: British Medical Journal Publishing Group
Section: Communication},
	keywords = {information technology, statistics \& research methods, systematic review},
	pages = {e072254},
}

@article{hagendorff_human-like_2023,
	title = {Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in {ChatGPT}},
	volume = {3},
	issn = {2662-8457},
	url = {https://www.nature.com/articles/s43588-023-00527-x},
	doi = {10.1038/s43588-023-00527-x},
	abstract = {Abstract
            We design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI’s generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input–output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics.},
	language = {en},
	number = {10},
	urldate = {2024-05-28},
	journal = {Nature Computational Science},
	author = {Hagendorff, Thilo and Fabi, Sarah and Kosinski, Michal},
	month = oct,
	year = {2023},
	pages = {833--838},
}

@misc{puigcerver_sparse_2024,
	title = {From {Sparse} to {Soft} {Mixtures} of {Experts}},
	url = {http://arxiv.org/abs/2308.00951},
	doi = {10.48550/arXiv.2308.00951},
	abstract = {Sparse mixture of expert architectures (MoEs) scale model capacity without significant increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose Soft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoEs, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity (and performance) at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms dense Transformers (ViTs) and popular MoEs (Tokens Choice and Experts Choice). Furthermore, Soft MoE scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40x more parameters than ViT Huge/14, with only 2\% increased inference time, and substantially better quality.},
	urldate = {2024-05-28},
	publisher = {arXiv},
	author = {Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Houlsby, Neil},
	month = may,
	year = {2024},
	note = {arXiv:2308.00951 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{radhakrishnan_question_nodate,
	title = {Question {Decomposition} {Improves} the {Faithfulness} of {Model}-{Generated} {Reasoning}},
	language = {en},
	author = {Radhakrishnan, Ansh and Nguyen, Karina and Chen, Anna and Chen, Carol and Denison, Carson and Hernandez, Danny and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and Lukošiūtė, Kamilė and Cheng, Newton and Joseph, Nicholas and Schiefer, Nicholas and Rausch, Oliver and McCandlish, Sam and Showk, Sheer El and Lanham, Tamera and Maxwell, Tim and Chandrasekaran, Venkatesa and Hatfield-Dodds, Zac and Kaplan, Jared and Brauner, Jan and Bowman, Samuel R and Perez, Ethan},
}

@article{schiavo_prospero_2019,
	title = {{PROSPERO}: {An} {International} {Register} of {Systematic} {Review} {Protocols}},
	volume = {38},
	issn = {0276-3869, 1540-9597},
	shorttitle = {{PROSPERO}},
	url = {https://www.tandfonline.com/doi/full/10.1080/02763869.2019.1588072},
	doi = {10.1080/02763869.2019.1588072},
	abstract = {PROSPERO is an international database of systematic review protocols produced by the University of York’s Center for Research and Dissemination and funded by the National Institute for Health Research. It contains protocols of systematic reviews on health and social care, welfare, public health, education, crime, justice, and health-related international development. PROSPERO compiles a comprehensive listing of systematic review protocols in an attempt to avoid duplication of effort, reduce reporting bias, and promote transparency.},
	language = {en},
	number = {2},
	urldate = {2024-11-10},
	journal = {Medical Reference Services Quarterly},
	author = {Schiavo, Julie H.},
	month = apr,
	year = {2019},
	pages = {171--180},
}

@software{Boero_prismAId_-_Open,
  author = {Boero, Riccardo},
  doi = {10.5281/zenodo.11210796},
  license = {AGPL-3.0-only},
  title = {{prismAId - Open Science AI Tools for Systematic, Protocol-Based Literature Reviews}},
	year = {2024},
  url = {https://github.com/open-and-sustainable/prismaid}
}

@misc{prismaid,
  author = {Boero, Riccardo},
  title = {{prismAId - Open Science AI Tools for Systematic, Protocol-Based Literature Reviews}},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/open-and-sustainable/prismaid}
}

@misc{prismaid-doc,
  author = {Boero, Riccardo},
  title = {{prismAId - Documentaiton website}},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub pages},
  url = {https://open-and-sustainable.github.io/prismaid/}
}

@article{boero_ai-enhanced_2024,
	title = {An {AI}-{Enhanced} {Systematic} {Review} of {Climate} {Adaptation} {Costs}: {Approaches} and {Advancements}, 2010–2021},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2225-1154},
	shorttitle = {An {AI}-{Enhanced} {Systematic} {Review} of {Climate} {Adaptation} {Costs}},
	url = {https://www.mdpi.com/2225-1154/12/8/116},
	doi = {10.3390/cli12080116},
	abstract = {This study addresses the critical global challenge of climate adaptation by assessing the inadequacies in current methodologies for estimating adaptation costs. Broad assessments reveal a significant investment shortfall in adaptation strategies, highlighting the necessity for precise cost analysis to guide effective policy-making. By employing the PRISMA 2020 protocol and enhancing it with the prismAId tool, this review systematically analyzes the recent evolution of cost assessment methodologies using state-of-the-art generative AI. The AI-enhanced approach facilitates rapid and replicable research extensions. The analysis reveals a significant geographical and sectoral disparity in research on climate adaptation costs, with notable underrepresentation of crucial areas and sectors that are most vulnerable to climate impacts. The study also highlights a predominant reliance on secondary data and a lack of comprehensive uncertainty quantification in economic assessments, suggesting an urgent need for methodological enhancements. It concludes that extending analyses beyond merely verifying that benefits exceed costs is crucial for supporting effective climate adaptation. By assessing the profitability of adaptation investments, it becomes possible to prioritize these investments not only against similar interventions but also across the broader spectrum of public spending.},
	language = {en},
	number = {8},
	urldate = {2024-08-14},
	journal = {Climate},
	author = {Boero, Riccardo},
	month = aug,
	year = {2024},
	pages = {116},
	file = {PDF:/var/home/ribo/Zotero/storage/EMFAJHKW/Boero - 2024 - An AI-Enhanced Systematic Review of Climate Adaptation Costs Approaches and Advancements, 2010–2021.pdf:application/pdf},
}