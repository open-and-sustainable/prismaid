@article{page_prisma_2021,
	title = {The {PRISMA} 2020 statement: an updated guideline for reporting systematic reviews},
	volume = {10},
	issn = {2046-4053},
	shorttitle = {The {PRISMA} 2020 statement},
	url = {https://doi.org/10.1186/s13643-021-01626-4},
	doi = {10.1186/s13643-021-01626-4},
	number = {1},
	urldate = {2022-11-03},
	journal = {Systematic Reviews},
	author = {Page, Matthew J. and McKenzie, Joanne E. and Bossuyt, Patrick M. and Boutron, Isabelle and Hoffmann, Tammy C. and Mulrow, Cynthia D. and Shamseer, Larissa and Tetzlaff, Jennifer M. and Akl, Elie A. and Brennan, Sue E. and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M. and Hróbjartsson, Asbjørn and Lalu, Manoj M. and Li, Tianjing and Loder, Elizabeth W. and Mayo-Wilson, Evan and McDonald, Steve and McGuinness, Luke A. and Stewart, Lesley A. and Thomas, James and Tricco, Andrea C. and Welch, Vivian A. and Whiting, Penny and Moher, David},
	month = mar,
	year = {2021},
	pages = {89},
}

@article{blaizot_using_2022,
	title = {Using artificial intelligence methods for systematic review in health sciences: {A} systematic review},
	volume = {13},
	issn = {1759-2887},
	shorttitle = {Using artificial intelligence methods for systematic review in health sciences},
	doi = {10.1002/jrsm.1553},
	abstract = {The exponential increase in published articles makes a thorough and expedient review of literature increasingly challenging. This review delineated automated tools and platforms that employ artificial intelligence (AI) approaches and evaluated the reported benefits and challenges in using such methods. A search was conducted in 4 databases (Medline, Embase, CDSR, and Epistemonikos) up to April 2021 for systematic reviews and other related reviews implementing AI methods. To be included, the review must use any form of AI method, including machine learning, deep learning, neural network, or any other applications used to enable the full or semi-autonomous performance of one or more stages in the development of evidence synthesis. Twelve reviews were included, using nine different tools to implement 15 different AI methods. Eleven methods were used in the screening stages of the review (73\%). The rest were divided: two in data extraction (13\%) and two in risk of bias assessment (13\%). The ambiguous benefits of the data extractions, combined with the reported advantages from 10 reviews, indicating that AI platforms have taken hold with varying success in evidence synthesis. However, the results are qualified by the reliance on the self-reporting of the review authors. Extensive human validation still appears required at this stage in implementing AI methods, though further evaluation is required to define the overall contribution of such platforms in enhancing efficiency and quality in evidence synthesis.},
	language = {eng},
	number = {3},
	journal = {Research Synthesis Methods},
	author = {Blaizot, Aymeric and Veettil, Sajesh K. and Saidoung, Pantakarn and Moreno-Garcia, Carlos Francisco and Wiratunga, Nirmalie and Aceves-Martins, Magaly and Lai, Nai Ming and Chaiyakunapruk, Nathorn},
	month = may,
	year = {2022},
	pmid = {35174972},
	keywords = {artificial intelligence, Artificial Intelligence, evidence synthesis, Humans, machine learning, Machine Learning, Medicine, systematic reviews, Systematic Reviews as Topic},
	pages = {353--362},
}

@article{fabiano_how_nodate,
	title = {How to optimize the systematic review process using {AI} tools},
	volume = {n/a},
	copyright = {© 2024 The Authors. JCPP Advances published by John Wiley \& Sons Ltd on behalf of Association for Child and Adolescent Mental Health.},
	issn = {2692-9384},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jcv2.12234},
	doi = {10.1002/jcv2.12234},
	abstract = {Systematic reviews are a cornerstone for synthesizing the available evidence on a given topic. They simultaneously allow for gaps in the literature to be identified and provide direction for future research. However, due to the ever-increasing volume and complexity of the available literature, traditional methods for conducting systematic reviews are less efficient and more time-consuming. Numerous artificial intelligence (AI) tools are being released with the potential to optimize efficiency in academic writing and assist with various stages of the systematic review process including developing and refining search strategies, screening titles and abstracts for inclusion or exclusion criteria, extracting essential data from studies and summarizing findings. Therefore, in this article we provide an overview of the currently available tools and how they can be incorporated into the systematic review process to improve efficiency and quality of research synthesis. We emphasize that authors must report all AI tools that have been used at each stage to ensure replicability as part of reporting in methods.},
	language = {en},
	number = {n/a},
	urldate = {2024-05-30},
	journal = {JCPP Advances},
	author = {Fabiano, Nicholas and Gupta, Arnav and Bhambra, Nishaant and Luu, Brandon and Wong, Stanley and Maaz, Muhammad and Fiedorowicz, Jess G. and Smith, Andrew L. and Solmi, Marco},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jcv2.12234},
	keywords = {artificial intelligence, ChatGPT, evidence synthesis, large-language models, systematic review},
	pages = {e12234},
	year = 2024
}

@article{van_de_schoot_open_2021,
	title = {An open source machine learning framework for efficient and transparent systematic reviews},
	volume = {3},
	copyright = {2021 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00287-7},
	doi = {10.1038/s42256-020-00287-7},
	abstract = {To help researchers conduct a systematic review or meta-analysis as efficiently and transparently as possible, we designed a tool to accelerate the step of screening titles and abstracts. For many tasks—including but not limited to systematic reviews and meta-analyses—the scientific literature needs to be checked systematically. Scholars and practitioners currently screen thousands of studies by hand to determine which studies to include in their review or meta-analysis. This is error prone and inefficient because of extremely imbalanced data: only a fraction of the screened studies is relevant. The future of systematic reviewing will be an interaction with machine learning algorithms to deal with the enormous increase of available text. We therefore developed an open source machine learning-aided pipeline applying active learning: ASReview. We demonstrate by means of simulation studies that active learning can yield far more efficient reviewing than manual reviewing while providing high quality. Furthermore, we describe the options of the free and open source research software and present the results from user experience tests. We invite the community to contribute to open source projects such as our own that provide measurable and reproducible improvements over current practice.},
	language = {en},
	number = {2},
	urldate = {2024-05-30},
	journal = {Nature Machine Intelligence},
	author = {van de Schoot, Rens and de Bruin, Jonathan and Schram, Raoul and Zahedi, Parisa and de Boer, Jan and Weijdema, Felix and Kramer, Bianca and Huijts, Martijn and Hoogerwerf, Maarten and Ferdinands, Gerbrich and Harkema, Albert and Willemsen, Joukje and Ma, Yongchao and Fang, Qixiang and Hindriks, Sybren and Tummers, Lars and Oberski, Daniel L.},
	month = feb,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Computer science, Medical research, SARS-CoV-2},
	pages = {125--133},
}

@article{de_la_torre-lopez_artificial_2023,
	title = {Artificial intelligence to automate the systematic review of scientific literature},
	volume = {105},
	issn = {1436-5057},
	url = {https://doi.org/10.1007/s00607-023-01181-x},
	doi = {10.1007/s00607-023-01181-x},
	abstract = {Artificial intelligence (AI) has acquired notorious relevance in modern computing as it effectively solves complex tasks traditionally done by humans. AI provides methods to represent and infer knowledge, efficiently manipulate texts and learn from vast amount of data. These characteristics are applicable in many activities that human find laborious or repetitive, as is the case of the analysis of scientific literature. Manually preparing and writing a systematic literature review (SLR) takes considerable time and effort, since it requires planning a strategy, conducting the literature search and analysis, and reporting the findings. Depending on the area under study, the number of papers retrieved can be of hundreds or thousands, meaning that filtering those relevant ones and extracting the key information becomes a costly and error-prone process. However, some of the involved tasks are repetitive and, therefore, subject to automation by means of AI. In this paper, we present a survey of AI techniques proposed in the last 15 years to help researchers conduct systematic analyses of scientific literature. We describe the tasks currently supported, the types of algorithms applied, and available tools proposed in 34 primary studies. This survey also provides a historical perspective of the evolution of the field and the role that humans can play in an increasingly automated SLR process.},
	language = {en},
	number = {10},
	urldate = {2024-05-30},
	journal = {Computing},
	author = {de la Torre-López, José and Ramírez, Aurora and Romero, José Raúl},
	month = oct,
	year = {2023},
	keywords = {68T01 General topics in artificial intelligence, Artificial intelligence, Machine learning, Survey, Systematic literature review},
	pages = {2171--2194},
}

@article{dijk_artificial_2023,
	title = {Artificial intelligence in systematic reviews: promising when appropriately used},
	volume = {13},
	copyright = {© Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY. Published by BMJ.. https://creativecommons.org/licenses/by/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution 4.0 Unported (CC BY 4.0) license, which permits others to copy, redistribute, remix, transform and build upon this work for any purpose, provided the original work is properly cited, a link to the licence is given, and indication of whether changes were made. See: https://creativecommons.org/licenses/by/4.0/.},
	issn = {2044-6055, 2044-6055},
	shorttitle = {Artificial intelligence in systematic reviews},
	url = {https://bmjopen.bmj.com/content/13/7/e072254},
	doi = {10.1136/bmjopen-2023-072254},
	abstract = {Background Systematic reviews provide a structured overview of the available evidence in medical-scientific research. However, due to the increasing medical-scientific research output, it is a time-consuming task to conduct systematic reviews. To accelerate this process, artificial intelligence (AI) can be used in the review process. In this communication paper, we suggest how to conduct a transparent and reliable systematic review using the AI tool ‘ASReview’ in the title and abstract screening.
Methods Use of the AI tool consisted of several steps. First, the tool required training of its algorithm with several prelabelled articles prior to screening. Next, using a researcher-in-the-loop algorithm, the AI tool proposed the article with the highest probability of being relevant. The reviewer then decided on relevancy of each article proposed. This process was continued until the stopping criterion was reached. All articles labelled relevant by the reviewer were screened on full text.
Results Considerations to ensure methodological quality when using AI in systematic reviews included: the choice of whether to use AI, the need of both deduplication and checking for inter-reviewer agreement, how to choose a stopping criterion and the quality of reporting. Using the tool in our review resulted in much time saved: only 23\% of the articles were assessed by the reviewer.
Conclusion The AI tool is a promising innovation for the current systematic reviewing practice, as long as it is appropriately used and methodological quality can be assured.
PROSPERO registration number CRD42022283952.},
	language = {en},
	number = {7},
	urldate = {2024-05-30},
	journal = {BMJ Open},
	author = {Dijk, Sanne H. B. van and Brusse-Keizer, Marjolein G. J. and Bucsán, Charlotte C. and Palen, Job van der and Doggen, Carine J. M. and Lenferink, Anke},
	month = jul,
	year = {2023},
	pmid = {37419641},
	note = {Publisher: British Medical Journal Publishing Group
Section: Communication},
	keywords = {information technology, statistics \& research methods, systematic review},
	pages = {e072254},
}

@article{schiavo_prospero_2019,
	title = {{PROSPERO}: {An} {International} {Register} of {Systematic} {Review} {Protocols}},
	volume = {38},
	issn = {0276-3869, 1540-9597},
	shorttitle = {{PROSPERO}},
	url = {https://www.tandfonline.com/doi/full/10.1080/02763869.2019.1588072},
	doi = {10.1080/02763869.2019.1588072},
	abstract = {PROSPERO is an international database of systematic review protocols produced by the University of York’s Center for Research and Dissemination and funded by the National Institute for Health Research. It contains protocols of systematic reviews on health and social care, welfare, public health, education, crime, justice, and health-related international development. PROSPERO compiles a comprehensive listing of systematic review protocols in an attempt to avoid duplication of effort, reduce reporting bias, and promote transparency.},
	language = {en},
	number = {2},
	urldate = {2024-11-10},
	journal = {Medical Reference Services Quarterly},
	author = {Schiavo, Julie H.},
	month = apr,
	year = {2019},
	pages = {171--180},
}

@software{Boero_prismAId_-_Open,
  author = {Boero, Riccardo},
  doi = {10.5281/zenodo.11210796},
  license = {AGPL-3.0-only},
  title = {{prismAId - Open Science AI Tools for Systematic, Protocol-Based Literature Reviews}},
	year = {2024},
  url = {https://github.com/open-and-sustainable/prismaid}
}

@misc{prismaid,
  author = {Boero, Riccardo},
  title = {{prismAId - Open Science AI Tools for Systematic, Protocol-Based Literature Reviews}},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/open-and-sustainable/prismaid}
}

@misc{prismaid-doc,
  author = {Boero, Riccardo},
  title = {{prismAId - Documentation website}},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub pages},
  url = {https://open-and-sustainable.github.io/prismaid/}
}

@article{boero_ai-enhanced_2024,
	title = {An {AI}-{Enhanced} {Systematic} {Review} of {Climate} {Adaptation} {Costs}: {Approaches} and {Advancements}, 2010–2021},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2225-1154},
	shorttitle = {An {AI}-{Enhanced} {Systematic} {Review} of {Climate} {Adaptation} {Costs}},
	url = {https://www.mdpi.com/2225-1154/12/8/116},
	doi = {10.3390/cli12080116},
	abstract = {This study addresses the critical global challenge of climate adaptation by assessing the inadequacies in current methodologies for estimating adaptation costs. Broad assessments reveal a significant investment shortfall in adaptation strategies, highlighting the necessity for precise cost analysis to guide effective policy-making. By employing the PRISMA 2020 protocol and enhancing it with the prismAId tool, this review systematically analyzes the recent evolution of cost assessment methodologies using state-of-the-art generative AI. The AI-enhanced approach facilitates rapid and replicable research extensions. The analysis reveals a significant geographical and sectoral disparity in research on climate adaptation costs, with notable underrepresentation of crucial areas and sectors that are most vulnerable to climate impacts. The study also highlights a predominant reliance on secondary data and a lack of comprehensive uncertainty quantification in economic assessments, suggesting an urgent need for methodological enhancements. It concludes that extending analyses beyond merely verifying that benefits exceed costs is crucial for supporting effective climate adaptation. By assessing the profitability of adaptation investments, it becomes possible to prioritize these investments not only against similar interventions but also across the broader spectrum of public spending.},
	language = {en},
	number = {8},
	urldate = {2024-08-14},
	journal = {Climate},
	author = {Boero, Riccardo},
	month = aug,
	year = {2024},
	pages = {116},
}

@misc{boero_extended_2024,
	title = {An {Extended} {Introduction} to the `{prismAId}` {Tool}: {Open}-{Source} and {Open} {Science} {AI} for {Systematic} {Reviews}},
	shorttitle = {An {Extended} {Introduction} to the `{prismAId}` {Tool}},
	url = {https://osf.io/wh8qn},
	doi = {10.31222/osf.io/wh8qn},
	abstract = {`prismAId` is an open-source tool designed to streamline systematic literature reviews by leveraging generative AI models for information extraction. It offers an accessible, efficient, and replicable method for extracting and analyzing data from scientific literature, eliminating the need for coding expertise. Supporting various review protocols, including PRISMA 2020, `prismAId` is distributed across multiple platforms — Go, Python, Julia, R — and provides user-friendly binaries compatible with Windows, macOS, and Linux. The tool integrates with leading large language models (LLMs) such as OpenAI's GPT series, Google's Gemini, Cohere's Command, and Anthropic's Claude, ensuring comprehensive and up-to-date literature analysis. `prismAId` facilitates systematic reviews, enabling researchers to conduct thorough, fast, and reproducible analyses, thereby advancing open science initiatives.},
	language = {en-us},
	urldate = {2024-12-05},
	publisher = {MetaArXiv},
	author = {Boero, Riccardo},
	month = nov,
	year = {2024},
	keywords = {Generative AI, Large Language Model, Open Source, Sopen Science, Systematic Literature Review},
}